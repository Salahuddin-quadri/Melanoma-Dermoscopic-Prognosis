{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference & Visualization Notebook\n",
        "\n",
        "This notebook loads the trained hybrid melanoma model, runs inference on the held-out test split, and generates several qualitative and quantitative visuals (ROC curve, probability histograms, regression scatter plots, and sample predictions with images). Update the configuration cell below to match the checkpoint and model variant you want to evaluate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    confusion_matrix,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    r2_score,\n",
        ")\n",
        "\n",
        "# Ensure the project root is on the Python path\n",
        "PROJECT_ROOT = Path.cwd().resolve().parent\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "from src.utils import load_emb_data, split_dataset, StructuredPreprocessor  # noqa: E402\n",
        "from src.evaluate import EvalDataset  # noqa: E402\n",
        "from src.models import create_dino_hybrid_model, create_hybrid_model  # noqa: E402\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Configuration ---- #\n",
        "metadata_path = PROJECT_ROOT / \"data/merged_dataset.csv\"\n",
        "image_dir = PROJECT_ROOT / \"data/images\"\n",
        "checkpoint_path = PROJECT_ROOT / \"outputs/train2/checkpoints/best.pt\"  # update as needed\n",
        "\n",
        "model_type = \"dino\"  # \"dino\" or \"resnet\"\n",
        "vit_arch = \"vit_b_32\"  # only used when model_type == \"dino\"\n",
        "use_tokens = True\n",
        "fusion_type = \"cross_attention\"\n",
        "num_clin_tokens = 4\n",
        "freeze_backbone_layers = 7\n",
        "\n",
        "image_size = (384, 384)\n",
        "batch_size = 16\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Task configuration\n",
        "multitask = True  # set False for classification-only models\n",
        "task = \"classification\"  # \"classification\" or \"regression\" (ignored when multitask=True)\n",
        "loss_alpha = 0.5  # used for combined metrics in multitask mode\n",
        "\n",
        "# Feature handling\n",
        "remove_ajcc = True  # drop AJCC/stage columns to avoid leakage with thickness\n",
        "\n",
        "# Visualization settings\n",
        "num_image_samples = 6  # number of images to visualize with predictions\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Load & preprocess dataset ---- #\n",
        "df = load_emb_data(metadata_path, image_dir)\n",
        "splits = split_dataset(df, val_size=0.15, test_size=0.15, stratify=True)\n",
        "\n",
        "target_col = \"label\" if (not multitask and task == \"classification\") else \"target\"\n",
        "if target_col not in splits.train.columns:\n",
        "    if multitask:\n",
        "        target_col = \"label\"\n",
        "    else:\n",
        "        target_col = \"thickness\" if task == \"regression\" else \"label\"\n",
        "\n",
        "for part in [splits.train, splits.val, splits.test]:\n",
        "    part.loc[:, \"target\"] = part[target_col]\n",
        "    if task == \"classification\" or multitask:\n",
        "        part.loc[:, \"target\"] = part[\"target\"].astype(float)\n",
        "\n",
        "exclude = {\"image_path\", \"image\", \"label\", \"target\", \"cathegory\", \"category\"}\n",
        "feature_cols = [c for c in splits.train.columns if c not in exclude]\n",
        "feature_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(splits.train[c])]\n",
        "\n",
        "if remove_ajcc:\n",
        "    ajcc_cols = [c for c in feature_cols if (\"ajcc\" in c.lower()) or c.lower().startswith(\"stage\")]\n",
        "    feature_cols = [c for c in feature_cols if c not in ajcc_cols]\n",
        "    if ajcc_cols:\n",
        "        print(f\"Removed AJCC columns: {ajcc_cols}\")\n",
        "\n",
        "sp = StructuredPreprocessor(feature_names=feature_cols)\n",
        "train_struct = sp.fit_transform(splits.train[feature_cols].to_numpy())\n",
        "val_struct = sp.transform(splits.val[feature_cols].to_numpy())\n",
        "test_struct = sp.transform(splits.test[feature_cols].to_numpy())\n",
        "\n",
        "for i, col in enumerate(feature_cols):\n",
        "    splits.train[col] = train_struct[:, i]\n",
        "    splits.val[col] = val_struct[:, i]\n",
        "    splits.test[col] = test_struct[:, i]\n",
        "\n",
        "print(f\"Structured feature count: {len(feature_cols)}\")\n",
        "test_df = splits.test.reset_index(drop=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Build dataloader ---- #\n",
        "test_dataset = EvalDataset(test_df, feature_cols=feature_cols, image_size=image_size)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=torch.cuda.is_available(),\n",
        ")\n",
        "\n",
        "# ---- Create & load model ---- #\n",
        "if model_type == \"dino\":\n",
        "    model = create_dino_hybrid_model(\n",
        "        num_structured_features=len(feature_cols),\n",
        "        task=task,\n",
        "        multitask=multitask,\n",
        "        arch=vit_arch,\n",
        "        pretrained=True,\n",
        "        dino_checkpoint=None,\n",
        "        use_tokens=use_tokens,\n",
        "        fusion_type=fusion_type,\n",
        "        num_clin_tokens=num_clin_tokens,\n",
        "        freeze_backbone_layers=freeze_backbone_layers,\n",
        "    )\n",
        "else:\n",
        "    model = create_hybrid_model(\n",
        "        num_structured_features=len(feature_cols),\n",
        "        task=task,\n",
        "        multitask=multitask,\n",
        "    )\n",
        "\n",
        "state = torch.load(checkpoint_path, map_location=device)\n",
        "if isinstance(state, dict) and \"model_state\" in state:\n",
        "    state = state[\"model_state\"]\n",
        "model.load_state_dict(state, strict=False)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Run inference ---- #\n",
        "cls_probs, cls_labels = [], []\n",
        "reg_preds, reg_targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        imgs = batch[\"img\"].to(device)\n",
        "        tabs = batch[\"tab\"].to(device)\n",
        "        outputs = model(imgs, tabs)\n",
        "\n",
        "        if isinstance(outputs, dict):\n",
        "            if \"cls\" in outputs:\n",
        "                prob = outputs[\"cls\"].detach().cpu().numpy().reshape(-1)\n",
        "                cls_probs.append(prob)\n",
        "                cls_labels.append(batch.get(\"label\", torch.zeros_like(outputs[\"cls\"])).numpy().reshape(-1))\n",
        "            if \"reg\" in outputs and \"thickness\" in batch:\n",
        "                reg_preds.append(outputs[\"reg\"].detach().cpu().numpy().reshape(-1))\n",
        "                reg_targets.append(batch[\"thickness\"].numpy().reshape(-1))\n",
        "        else:\n",
        "            prob = outputs.detach().cpu().numpy().reshape(-1)\n",
        "            cls_probs.append(prob)\n",
        "            cls_labels.append(batch.get(\"label\", torch.zeros_like(outputs)).numpy().reshape(-1))\n",
        "\n",
        "cls_probs = np.concatenate(cls_probs) if cls_probs else None\n",
        "cls_labels = np.concatenate(cls_labels) if cls_labels else None\n",
        "reg_preds = np.concatenate(reg_preds) if reg_preds else None\n",
        "reg_targets = np.concatenate(reg_targets) if reg_targets else None\n",
        "\n",
        "print(f\"Collected predictions for {len(test_df)} samples\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Compute metrics ---- #\n",
        "metrics = {}\n",
        "if cls_probs is not None and cls_labels is not None:\n",
        "    y_pred_binary = (cls_probs >= 0.5).astype(int)\n",
        "    metrics[\"accuracy\"] = accuracy_score(cls_labels, y_pred_binary)\n",
        "    metrics[\"precision\"] = precision_score(cls_labels, y_pred_binary, zero_division=0)\n",
        "    metrics[\"recall\"] = recall_score(cls_labels, y_pred_binary, zero_division=0)\n",
        "    metrics[\"f1\"] = f1_score(cls_labels, y_pred_binary, zero_division=0)\n",
        "    try:\n",
        "        metrics[\"auroc\"] = roc_auc_score(cls_labels, cls_probs)\n",
        "    except ValueError:\n",
        "        metrics[\"auroc\"] = np.nan\n",
        "\n",
        "    print(\"Classification metrics:\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"  {k:10s}: {v:.4f}\")\n",
        "\n",
        "if reg_preds is not None and reg_targets is not None:\n",
        "    reg_metrics = {\n",
        "        \"mae\": mean_absolute_error(reg_targets, reg_preds),\n",
        "        \"rmse\": np.sqrt(mean_squared_error(reg_targets, reg_preds)),\n",
        "        \"r2\": r2_score(reg_targets, reg_preds),\n",
        "    }\n",
        "    print(\"\\nRegression metrics:\")\n",
        "    for k, v in reg_metrics.items():\n",
        "        print(f\"  {k:10s}: {v:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Visuals: ROC curve & probability histogram ---- #\n",
        "if cls_probs is not None and cls_labels is not None:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # ROC curve\n",
        "    try:\n",
        "        fpr, tpr, _ = roc_curve(cls_labels, cls_probs)\n",
        "        axes[0].plot(fpr, tpr, label=f\"AUROC = {metrics.get('auroc', float('nan')):.3f}\")\n",
        "        axes[0].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
        "        axes[0].set_xlabel(\"False Positive Rate\")\n",
        "        axes[0].set_ylabel(\"True Positive Rate\")\n",
        "        axes[0].set_title(\"ROC Curve\")\n",
        "        axes[0].legend(loc=\"lower right\")\n",
        "    except ValueError:\n",
        "        axes[0].text(0.5, 0.5, \"ROC undefined\", ha=\"center\")\n",
        "\n",
        "    # Probability histogram\n",
        "    axes[1].hist(cls_probs[cls_labels == 0], bins=20, alpha=0.6, label=\"Benign\")\n",
        "    axes[1].hist(cls_probs[cls_labels == 1], bins=20, alpha=0.6, label=\"Malignant\")\n",
        "    axes[1].axvline(0.5, color=\"red\", linestyle=\"--\", label=\"Threshold\")\n",
        "    axes[1].set_xlabel(\"Predicted probability\")\n",
        "    axes[1].set_ylabel(\"Count\")\n",
        "    axes[1].set_title(\"Prediction Distribution\")\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Visuals: Regression scatter ---- #\n",
        "if reg_preds is not None and reg_targets is not None:\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    ax.scatter(reg_targets, reg_preds, alpha=0.6)\n",
        "    min_val = min(reg_targets.min(), reg_preds.min())\n",
        "    max_val = max(reg_targets.max(), reg_preds.max())\n",
        "    ax.plot([min_val, max_val], [min_val, max_val], linestyle=\"--\", color=\"red\")\n",
        "    ax.set_xlabel(\"True thickness (mm)\")\n",
        "    ax.set_ylabel(\"Predicted thickness (mm)\")\n",
        "    ax.set_title(f\"Regression Scatter (RÂ²={r2_score(reg_targets, reg_preds):.3f})\")\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Assemble results dataframe ---- #\n",
        "results = test_df.copy()\n",
        "if cls_probs is not None:\n",
        "    results[\"pred_prob\"] = cls_probs\n",
        "    results[\"pred_label\"] = (cls_probs >= 0.5).astype(int)\n",
        "if reg_preds is not None:\n",
        "    results[\"pred_thickness\"] = reg_preds\n",
        "\n",
        "results.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Sample predictions with visuals ---- #\n",
        "def show_samples(df, title, selector):\n",
        "    subset = selector(df)\n",
        "    if subset.empty:\n",
        "        print(f\"No samples available for {title}\")\n",
        "        return\n",
        "    samples = subset.head(num_image_samples)\n",
        "\n",
        "    n_cols = min(3, num_image_samples)\n",
        "    n_rows = int(np.ceil(len(samples) / n_cols))\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
        "    axes = np.array(axes).reshape(-1)\n",
        "\n",
        "    for ax, (_, row) in zip(axes, samples.iterrows()):\n",
        "        try:\n",
        "            img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
        "            ax.imshow(img)\n",
        "            title_lines = [Path(row[\"image_path\"]).name]\n",
        "            if \"label\" in row:\n",
        "                title_lines.append(f\"GT label: {int(row['label'])}\")\n",
        "            if \"pred_prob\" in row:\n",
        "                title_lines.append(f\"Pred prob: {row['pred_prob']:.2f}\")\n",
        "            if \"thickness\" in row:\n",
        "                title_lines.append(f\"GT thick: {row['thickness']:.2f}mm\")\n",
        "            if \"pred_thickness\" in row:\n",
        "                title_lines.append(f\"Pred thick: {row['pred_thickness']:.2f}mm\")\n",
        "            ax.set_title(\"\\n\".join(title_lines), fontsize=9)\n",
        "            ax.axis(\"off\")\n",
        "        except Exception as exc:\n",
        "            ax.text(0.5, 0.5, str(exc), ha=\"center\")\n",
        "            ax.axis(\"off\")\n",
        "\n",
        "    for ax in axes[len(samples):]:\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    fig.suptitle(title, fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if cls_probs is not None:\n",
        "    show_samples(results.sort_values(\"pred_prob\", ascending=False), \"Top malignant predictions\", lambda df: df.sort_values(\"pred_prob\", ascending=False))\n",
        "    show_samples(results.sort_values(\"pred_prob\", ascending=True), \"Most confidently benign\", lambda df: df.sort_values(\"pred_prob\", ascending=True))\n",
        "\n",
        "if reg_preds is not None:\n",
        "    show_samples(results.reindex((reg_preds - reg_targets).argsort()[::-1]), \"Largest thickness overestimates\", lambda df: df.assign(err=df[\"pred_thickness\"] - df[\"thickness\"]).sort_values(\"err\", ascending=False))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Confusion matrix ---- #\n",
        "if cls_probs is not None and cls_labels is not None:\n",
        "    cm = confusion_matrix(cls_labels, (cls_probs >= 0.5).astype(int))\n",
        "    fig, ax = plt.subplots(figsize=(4, 4))\n",
        "    im = ax.imshow(cm, cmap=\"Blues\")\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"True\")\n",
        "    ax.set_title(\"Confusion Matrix\")\n",
        "    ax.set_xticks([0, 1])\n",
        "    ax.set_yticks([0, 1])\n",
        "    plt.colorbar(im, ax=ax)\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Optional: Save results to disk ---- #\n",
        "results_path = PROJECT_ROOT / \"outputs\" / \"inference_results.csv\"\n",
        "results.to_csv(results_path, index=False)\n",
        "print(f\"Saved detailed predictions to {results_path}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
