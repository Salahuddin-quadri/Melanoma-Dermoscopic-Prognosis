{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Melanoma Dermoscopic Prognosis - Colab Training Notebook\n",
        "\n",
        "This notebook sets up the environment, downloads data from Google Drive, and trains the model using `main.py`.\n",
        "\n",
        "**Note:** The model uses image-only input (dermoscopic images). Clinical features are not required.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Clone Repository\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Clone the repository\n",
        "repo_url = \"https://github.com/Salahuddin-quadri/Melanoma-Dermoscopic-Prognosis.git\"\n",
        "repo_name = \"Melanoma-Dermoscopic-Prognosis\"\n",
        "\n",
        "# Check if we're already in the repo directory\n",
        "if os.path.exists(\"src/main.py\"):\n",
        "    print(\"Already in repository directory. Skipping clone.\")\n",
        "    print(f\"Current directory: {os.getcwd()}\")\n",
        "elif os.path.exists(repo_name):\n",
        "    print(f\"Repository {repo_name} already exists. Changing to it.\")\n",
        "    os.chdir(repo_name)\n",
        "    print(f\"Current directory: {os.getcwd()}\")\n",
        "else:\n",
        "    # Clone the repository\n",
        "    get_ipython().system(f'git clone {repo_url}')\n",
        "    os.chdir(repo_name)\n",
        "    print(f\"Cloned and changed to: {os.getcwd()}\")\n",
        "\n",
        "# Verify we're in the right place\n",
        "assert os.path.exists(\"src/main.py\"), \"src/main.py not found! Check repository structure.\"\n",
        "print(\"✓ Repository setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install PyTorch with CUDA support\n",
        "!pip install torch==2.3.1 torchvision==0.18.1 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Install other dependencies\n",
        "!pip install numpy==1.26.4 pandas==2.2.2 scikit-learn==1.4.2 scipy==1.11.4\n",
        "!pip install matplotlib==3.8.4 seaborn==0.13.2 opencv-python==4.10.0.84 Pillow==10.4.0\n",
        "!pip install tqdm==4.66.4 ipywidgets==8.1.3 imbalanced-learn==0.12.3\n",
        "!pip install gdown\n",
        "\n",
        "print(\"✓ Dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Mount Google Drive and Download Data\n",
        "\n",
        "**Important:** The data files are large (~1GB). We'll mount Google Drive and download directly from the shared folders.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"✓ Google Drive mounted successfully!\")\n",
        "print(\"Your Drive is now accessible at /content/drive/MyDrive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download Data Folder from Google Drive\n",
        "\n",
        "The data folder contains:\n",
        "- `images/` - dermoscopic images\n",
        "- `merged_dataset.csv` - metadata CSV\n",
        "- `meta_data.csv` - additional metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the file from Google Drive\n",
        "file_id = \"1LCbiPcQXJperjrOhG4Xb947rkCiJPDRh\"\n",
        "output_filename = \"downloaded_file.zip\"\n",
        "\n",
        "!gdown {file_id} -O {output_filename}\n",
        "\n",
        "# Unzip only the 'data/' folder into the current directory\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "with zipfile.ZipFile(output_filename, 'r') as zip_ref:\n",
        "    members = [m for m in zip_ref.namelist() if m.startswith(\"data/\")]\n",
        "    zip_ref.extractall(members=members)\n",
        "\n",
        "print(\"✓ 'data/' folder extracted to current directory.\")\n",
        "print(\"\\nExtracted contents of 'data/':\")\n",
        "!ls -la data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download DINO v3 Pretrained Model from Google Drive\n",
        "\n",
        "The dino_v3 folder contains the pretrained checkpoint needed for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Drive folder ID for dino_v3 model\n",
        "dino_folder_id = \"1wlozH7SchoPkAgsDpn-DyBN-F2Ea8hmP\"\n",
        "\n",
        "# Create dino_v3 directory\n",
        "dino_dest = \"dino_v3\"\n",
        "os.makedirs(dino_dest, exist_ok=True)\n",
        "\n",
        "print(f\"Downloading dino_v3 folder (ID: {dino_folder_id})...\")\n",
        "print(\"This may take a while as the folder is large...\")\n",
        "\n",
        "# Use gdown to download the entire folder\n",
        "get_ipython().system(f'gdown --folder https://drive.google.com/drive/folders/{dino_folder_id} -O {dino_dest} --remaining-ok')\n",
        "\n",
        "print(\"\\n✓ DINO v3 model download complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify Downloaded Files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify data files\n",
        "data_dir = Path(\"data\")\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA FOLDER VERIFICATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if data_dir.exists():\n",
        "    # Calculate total size\n",
        "    total_size_mb = sum(f.stat().st_size for f in data_dir.rglob('*') if f.is_file()) / (1024 * 1024)\n",
        "    print(f\"Total size: {total_size_mb:.2f} MB\")\n",
        "    print(f\"\\nContents:\")\n",
        "    \n",
        "    # Check for required files\n",
        "    required_files = [\"merged_dataset.csv\", \"meta_data.csv\"]\n",
        "    for file in required_files:\n",
        "        file_path = data_dir / file\n",
        "        if file_path.exists():\n",
        "            size_mb = file_path.stat().st_size / (1024 * 1024)\n",
        "            print(f\"  ✓ {file} ({size_mb:.2f} MB)\")\n",
        "        else:\n",
        "            print(f\"  ✗ Missing: {file}\")\n",
        "    \n",
        "    # Check for images folder\n",
        "    images_dir = data_dir / \"images\"\n",
        "    if images_dir.exists():\n",
        "        num_images = len([f for f in images_dir.rglob(\"*\") if f.is_file()])\n",
        "        images_size_mb = sum(f.stat().st_size for f in images_dir.rglob(\"*\") if f.is_file()) / (1024 * 1024)\n",
        "        print(f\"  ✓ images/ folder ({num_images} files, {images_size_mb:.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"  ✗ Missing: images/ folder\")\n",
        "else:\n",
        "    print(\"✗ Data directory not found!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DINO_V3 FOLDER VERIFICATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Verify dino_v3 files\n",
        "dino_dir = Path(\"dino_v3\")\n",
        "if dino_dir.exists():\n",
        "    total_size_mb = sum(f.stat().st_size for f in dino_dir.rglob('*') if f.is_file()) / (1024 * 1024)\n",
        "    print(f\"Total size: {total_size_mb:.2f} MB\")\n",
        "    print(f\"\\nContents:\")\n",
        "    \n",
        "    # Check for checkpoint\n",
        "    checkpoint_path = dino_dir / \"outputs_dino\" / \"checkpoints\" / \"best.pt\"\n",
        "    if checkpoint_path.exists():\n",
        "        size_mb = checkpoint_path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"  ✓ {checkpoint_path.relative_to(dino_dir)} ({size_mb:.2f} MB)\")\n",
        "    else:\n",
        "        # Try alternative paths\n",
        "        checkpoints = list(dino_dir.rglob(\"*.pt\"))\n",
        "        if checkpoints:\n",
        "            for ckpt in checkpoints:\n",
        "                size_mb = ckpt.stat().st_size / (1024 * 1024)\n",
        "                print(f\"  ✓ {ckpt.relative_to(dino_dir)} ({size_mb:.2f} MB)\")\n",
        "        else:\n",
        "            print(f\"  ✗ No checkpoint files (.pt) found\")\n",
        "    \n",
        "    # List all files\n",
        "    print(f\"\\nAll files in dino_v3:\")\n",
        "    for item in sorted(dino_dir.rglob(\"*\")):\n",
        "        if item.is_file():\n",
        "            size_mb = item.stat().st_size / (1024 * 1024)\n",
        "            print(f\"  {item.relative_to(dino_dir)} ({size_mb:.2f} MB)\")\n",
        "else:\n",
        "    print(\"✗ dino_v3 directory not found!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Verify Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Python version\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# Check PyTorch\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "# Check repository structure\n",
        "print(\"\\nRepository structure:\")\n",
        "print(f\"  Current directory: {os.getcwd()}\")\n",
        "print(f\"  src/main.py exists: {os.path.exists('src/main.py')}\")\n",
        "print(f\"  requirements.txt exists: {os.path.exists('requirements.txt')}\")\n",
        "\n",
        "print(\"\\n✓ Setup verification complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Training Configuration\n",
        "\n",
        "Configure your training parameters here. Adjust as needed.\n",
        "\n",
        "**Model Architecture:** The model takes only dermoscopic images as input (no clinical features). \n",
        "- DINO model: Uses Vision Transformer (ViT) backbone with domain-specific pretraining\n",
        "- ResNet model: Uses ResNet50 backbone with ImageNet pretraining\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "# Note: Model takes only images as input (no clinical features)\n",
        "config = {\n",
        "    \"metadata_path\": \"data/meta_data.csv\",\n",
        "    \"image_dir\": \"data/images\",\n",
        "    \"mode\": \"train\",\n",
        "    \"model_type\": \"dino\",  # \"dino\" or \"resnet\"\n",
        "    \"epochs\": 200,\n",
        "    \"batch_size\": 32,\n",
        "    \"image_size\": [224,224],\n",
        "    \"output_dir\": \"outputs\",\n",
        "    \"task\": \"classification\",  # \"classification\" or \"regression\" (used when multitask=False)\n",
        "    \"multitask\": True,  # Set to True for dual-head (classification + regression)\n",
        "    \"loss_alpha\": 0.5,  # Weight for classification loss in multitask (0-1)\n",
        "    \"cls_loss_type\": \"weighted_bce\",  # \"bce\", \"weighted_bce\", or \"focal\"\n",
        "    \"focal_gamma\": 2.0,  # For focal loss (used when cls_loss_type=\"focal\")\n",
        "    \"freeze_backbone_layers\": 7,  # Number of ViT layers to freeze (0 = all trainable)\n",
        "    \"val_size\": 0.15,\n",
        "    \"test_size\": 0.15,\n",
        "    \"device\": \"auto\",  # \"cuda\", \"cpu\", or \"auto\"\n",
        "}\n",
        "\n",
        "# Set DINO checkpoint path\n",
        "# Try to find the checkpoint automatically\n",
        "dino_checkpoint_candidates = [\n",
        "    \"dino_v3/outputs_dino/checkpoints/best.pt\",\n",
        "    \"dino_v3/checkpoints/best.pt\",\n",
        "]\n",
        "\n",
        "dino_checkpoint = None\n",
        "for candidate in dino_checkpoint_candidates:\n",
        "    if os.path.exists(candidate):\n",
        "        dino_checkpoint = candidate\n",
        "        break\n",
        "\n",
        "if dino_checkpoint:\n",
        "    config[\"dino_checkpoint\"] = dino_checkpoint\n",
        "    print(f\"✓ Found DINO checkpoint: {dino_checkpoint}\")\n",
        "else:\n",
        "    print(\"⚠ No DINO checkpoint found. Will use ImageNet pretrained weights.\")\n",
        "    # Don't set dino_checkpoint if not found\n",
        "\n",
        "print(\"\\nTraining configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Run Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build command arguments\n",
        "args_list = []\n",
        "for key, value in config.items():\n",
        "    if value is not None and value != \"\":\n",
        "        if isinstance(value, bool):\n",
        "            if value:\n",
        "                args_list.append(f\"--{key}\")\n",
        "        elif isinstance(value, list):\n",
        "            args_list.append(f\"--{key}\")\n",
        "            args_list.extend([str(v) for v in value])\n",
        "        else:\n",
        "            args_list.append(f\"--{key}\")\n",
        "            args_list.append(str(value))\n",
        "\n",
        "# Convert to string\n",
        "args_str = \" \".join(args_list)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING COMMAND\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"python -m src.main {args_str}\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nStarting training...\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training\n",
        "get_ipython().system(f'python -m src.main {args_str}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List output files\n",
        "# Note: Training creates subdirectories train1, train2, etc. (YOLO-style organization)\n",
        "output_dir = Path(config[\"output_dir\"])\n",
        "if output_dir.exists():\n",
        "    print(f\"Output directory: {output_dir}\")\n",
        "    print(\"\\nContents:\")\n",
        "    \n",
        "    # Find training run directories (train1, train2, etc.)\n",
        "    train_dirs = sorted([d for d in output_dir.iterdir() if d.is_dir() and d.name.startswith(\"train\")])\n",
        "    \n",
        "    if train_dirs:\n",
        "        latest_train_dir = train_dirs[-1]\n",
        "        print(f\"\\nLatest training run: {latest_train_dir.name}\")\n",
        "        \n",
        "        # List files in latest training run\n",
        "        for item in latest_train_dir.rglob(\"*\"):\n",
        "            if item.is_file():\n",
        "                size = item.stat().st_size / (1024 * 1024)  # Size in MB\n",
        "                print(f\"  {item.relative_to(output_dir)} ({size:.2f} MB)\")\n",
        "        \n",
        "        # Check for checkpoints in latest training run\n",
        "        checkpoint_dir = latest_train_dir / \"checkpoints\"\n",
        "        if checkpoint_dir.exists():\n",
        "            print(f\"\\nCheckpoints in {latest_train_dir.name}:\")\n",
        "            for ckpt in checkpoint_dir.glob(\"*.pt\"):\n",
        "                size = ckpt.stat().st_size / (1024 * 1024)\n",
        "                print(f\"  ✓ {ckpt.name} ({size:.2f} MB)\")\n",
        "        else:\n",
        "            print(f\"\\n⚠ No checkpoints directory found in {latest_train_dir.name}\")\n",
        "    else:\n",
        "        # Fallback: list all files directly\n",
        "        for item in output_dir.rglob(\"*\"):\n",
        "            if item.is_file():\n",
        "                size = item.stat().st_size / (1024 * 1024)  # Size in MB\n",
        "                print(f\"  {item.relative_to(output_dir)} ({size:.2f} MB)\")\n",
        "else:\n",
        "    print(f\"⚠ Output directory {output_dir} not found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Save Results to Google Drive\n",
        "\n",
        "After training, you can save the outputs to your Google Drive for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
