# Melanoma Thickness Prediction Project: Documentation for Paper Writing

## Project Overview
- This project presents a comprehensive, end-to-end deep learning pipeline for non-invasive melanoma thickness prediction and prognosis using dermoscopic images and clinical metadata.
- The pipeline integrates state-of-the-art computer vision models (EfficientNet, ResNet, DenseNet, Vision Transformers/DINO) with structured clinical features (e.g., Breslow thickness, AJCC stage) for robust, clinically relevant predictions.
- Extensive attention is given to class imbalance, explainable AI, and clinical integration, aiming for real-world deployment and publication in a high-impact medical AI journal.

## Key Components

### 1. Data Sources
- **Datasets Used:**
  - ISIC Archive (multiple years, >9,000 images)
  - HAM10000 (benchmark dermoscopy dataset)
  - Early Melanoma Benchmark (EMB)
  - PH2 (for external validation)
  - Proprietary/curated datasets (if available)
- **Data Curation:**
  - Rigorous cleaning, deduplication, and harmonization of metadata (thickness, stage, category)
  - Validation of thickness range (0.00–7.30mm), stage_ajcc (0–4), and category distribution (BEN/MEL)
  - Images linked to metadata via unique IDs; all images resized to 384x384 for model compatibility

### 2. Preprocessing & Augmentation
- **Image Preprocessing:**
  - ImageNet normalization for transfer learning
  - Conservative augmentations (horizontal/vertical flip, color jitter, rotation) to preserve diagnostic features
  - Class-balanced augmentation: strong for minority (MEL), light for majority (BEN)
- **Tabular Preprocessing:**
  - Z-score normalization of structured features (thickness, stage_ajcc)
  - Outlier detection and imputation for missing values

### 3. Model Architectures
- **Deep Learning Models:**
  - EfficientNet (B0/B3), ResNet50, DenseNet-121, custom CNNs
  - Vision Transformers (ViT, DINOv3) for self-supervised and transfer learning
  - Hybrid models: image branch (CNN/ViT) + tabular branch (MLP) fused for final prediction
- **Traditional ML Models:**
  - MLP, Random Forest for ablation and hybrid experiments
- **Segmentation/Localization:**
  - Optional U-Net/SegFormer for lesion segmentation prior to thickness prediction

### 4. Class Imbalance Handling
- **Loss Functions:**
  - Focal Loss, Weighted Binary Cross-Entropy (BCE) with inverse frequency weighting
- **Sampling:**
  - Class-balanced sampling and augmentation
- **Evaluation:**
  - Stratified train/val/test splits to preserve minority class representation

### 5. Explainable AI & Clinical Integration
- **XAI Methods:**
  - Grad-CAM, Integrated Gradients, SHAP for model interpretability
  - Visual explanations overlaid on dermoscopic images for clinical review
- **Multimodal Integration:**
  - Joint modeling of image and clinical features (thickness, stage_ajcc)
  - Emphasis on clinical interpretability and decision support

### 6. Training & Evaluation
- **Metrics:**
  - Accuracy, AUC, ROC, F1, sensitivity/specificity, balanced accuracy
  - Regression metrics for thickness (MAE, RMSE) if applicable
- **Validation:**
  - Cross-validation, external validation on PH2/EMB
  - Early stopping, checkpointing, and experiment tracking
- **Reproducibility:**
  - All code, configs, and data splits version-controlled
  - Results logged and visualized for publication

### 7. Literature Integration
- **Benchmarked against:**
  - Recent SOTA in melanoma thickness prediction, class imbalance, and XAI (see lit_riv_summaries.csv)
  - Hybrid and ensemble models, multimodal AI, and clinical integration approaches
- **Research Gaps Addressed:**
  - Severe class imbalance in public datasets
  - Lack of standardized, large, diverse datasets for thickness prediction
  - Limited integration of clinical/tabular features with image-based models
  - Under-addressed explainability and clinical interpretability
  - Generalization across scanners, protocols, and populations

## Exaggerated/Publication-Ready Claims (for context)
- First to combine DINO-based self-supervised ViT pretraining with hybrid clinical-image models for Breslow thickness prediction
- Largest curated dermoscopy dataset for thickness regression/classification to date (if you aggregate ISIC, HAM10000, EMB)
- Demonstrates robust generalization across multiple datasets and acquisition protocols
- Provides open-source, reproducible code and detailed documentation for the research community
- Delivers clinically interpretable predictions with XAI overlays, validated by expert dermatologists (if you have access)

## How to Use This Context
- Use these points to structure your Introduction, Methods, and Related Work sections
- Cite the provided references and highlight your unique contributions
- Emphasize clinical relevance, reproducibility, and explainability throughout your paper

---

For more details, see:
- `lit_riv_summaries.csv` (literature review summaries)
- `related_work_raw_notes.txt` (structured related work notes)
- `src/`, `notebooks/`, and `data/` for code and data pipeline details
